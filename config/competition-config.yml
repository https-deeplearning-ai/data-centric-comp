# Allows at most 5 submissions per user per period, where period is 24 hours by default.
max_submissions_per_period: 5
refresh_period_seconds: 604800
max_submissions_total: 65

# UUID of the worksheet where prediction and evaluation bundles are created for submissions.
log_worksheet_uuid: '0x37a5b4d4ec494cdeaedc73850336b8c1'    

# Configure the tag that participants use to submit to the competition.
# In this example, any bundle with the tag `some-competition-submit` would be
# considered as an official submission.
submission_tag: mnist-roman

# Configure how to mimic the submitted prediction bundles. When evaluating a submission, 
# `new` bundle will replace `old` bundle.
# For a machine learning competition, `old` bundle might be the dev set and `new` bundle
# might be the hidden test set.
predict:
  mimic:
    - {new: '0x0b792c7dbeb14c82bf98978389a8eb85', old: '0xe1d24af2ca5e4d95be653f1c7877125e'}
  tag: predict-mnist-roman

# Configure how to evaluate the new prediction bundles.
# In this example, evaluate.py is script that takes in the paths of the test labels and 
# predicted labels and outputs the evaluation results.
evaluate:
  # Essentially
  #     cl run evaluate.py:0x089063eb85b64b239b342405b5ebab57 \
  #            test.json:0x5538cba32e524fad8b005cd19abb9f95 \
  #            predictions.json:{predict}/predictions.json --- \
  #            python evaluate.py test.json predictions.json
  # where {predict} gets filled in with the uuid of the mimicked bundle above.
  dependencies:
  - {child_path: evaluate.py, parent_uuid: '0x045df721ff2a436d814a011421f7a340'}
  - {child_path: test, parent_uuid: '0x0b792c7dbeb14c82bf98978389a8eb85'}
  - {child_path: predictions.json, parent_uuid: '{predict}'}
  command: python evaluate.py predictions.json test
  tag: eval-mnist-roman
  metadata:
    request_docker_image: jupyter/tensorflow-notebook

# Define how to extract the scores from the evaluation bundle.
# In this example, result.json is a JSON file outputted from the evaluation step
# with F1 and exact match metrics (e.g. {"f1": 91, "exact_match": 92}).
score_specs:
- {key: '/result.json:accuracy', name: accuracy}